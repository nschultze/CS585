{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1- Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Positive Data\n",
    "positiveFilePath = \"clickbait.txt\"\n",
    "positiveList = []\n",
    "with open(positiveFilePath, \"r\") as file:\n",
    "    for line in file:\n",
    "        stripped_line = line.strip()\n",
    "        positiveList.append(stripped_line)\n",
    "#print(positiveList)\n",
    "#print(len(positiveList))\n",
    "\n",
    "#Importing Negative Data\n",
    "negativeFilePath = \"not-clickbait.txt\"\n",
    "negativeList = []\n",
    "with open(negativeFilePath, \"r\") as file: \n",
    "    for line in file:\n",
    "        stripped_line = line.strip()\n",
    "        negativeList.append(stripped_line)\n",
    "#print(negativeList)\n",
    "#print(len(negativeList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data target:  1910\n",
      "Testing data size:  238\n",
      "Validation data size:  240\n",
      "Training data target rate:  651\n",
      "Testing data target rate:  81\n",
      "Validation data target rate:  82\n"
     ]
    }
   ],
   "source": [
    "#Shuffling the 2 Lists Together\n",
    "temp = positiveList + negativeList\n",
    "label1 = 1\n",
    "label2 = 0\n",
    "merged_labels = [label1 if idx < len(positiveList) else label2 for idx in range(len(temp))]\n",
    "\n",
    "combined_lists = list(zip(merged_labels, temp))\n",
    "np.random.shuffle(combined_lists)\n",
    "shuffledLabels, shuffledData = zip(*combined_lists)\n",
    "size = len(temp)\n",
    "#print(size)\n",
    "\n",
    "#Splitting the Data into Train, Test, and Val\n",
    "trainRatio = 0.8\n",
    "testRatio = 0.1\n",
    "valRatio = 0.1\n",
    "\n",
    "trainSize = int(size * trainRatio)\n",
    "testSize = int(size * testRatio)\n",
    "valSize = int(size * valRatio)\n",
    "\n",
    "trainData = shuffledData[:trainSize]\n",
    "testData = shuffledData[trainSize:trainSize + testSize]\n",
    "valData = shuffledData[trainSize + testSize:]\n",
    "trainLabel = shuffledLabels[:trainSize]\n",
    "testLabel = shuffledLabels[trainSize:trainSize + testSize]\n",
    "valLabel = shuffledLabels[trainSize + testSize:]\n",
    "\n",
    "trainingDataSize = len(trainData)\n",
    "testingDataSize = len(testData)\n",
    "valDataSize = len(valData)\n",
    "positiveListSize = len(positiveList)\n",
    "negativeListSize = len(negativeList)\n",
    "wholeDataSize = len(positiveList) + len(negativeList)\n",
    "\n",
    "print(f\"Training data target: \", trainingDataSize)\n",
    "print(f\"Testing data size: \", testingDataSize)\n",
    "print(f\"Validation data size: \", valDataSize)\n",
    "print(f\"Training data target rate: \", round((positiveListSize / wholeDataSize) * trainingDataSize))\n",
    "print(f\"Testing data target rate: \", round((positiveListSize / wholeDataSize) * testingDataSize))\n",
    "print(f\"Validation data target rate: \", round((positiveListSize / wholeDataSize) * valDataSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PROBLEM 2 – Baseline Performance (10 pts – Answer in Blackboard)\n",
    "Assume you have a trivial baseline classifier that flags every text presented to it as clickbait. What is the precision, recall, and F1-score of such a classifier on your test set? Do you think there is another good baseline classifier that would give you higher F-1 score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision=((TP)/(TP+FP))=((651)/(651+1259))=0.341 because there are 651 is the number of True Positive and 1259 is the number of False Positives.\n",
    "\n",
    "Recall=((TP)/(TP+FN))=((651)/(651+0))=1 because 651 is the number of True Positives and 0 is the number of False Negatives.\n",
    "\n",
    "F-1 Score=((2xPxR)/(P+R))=((2x0.341x1)/(0.341+1))=0.501 because Precision is 0.341 and Recall is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (((651) / (651 + 1259)))\n",
    "#print(p)\n",
    "f = ((2 * p * 1)/ (p + 1))\n",
    "#print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 3 – Training a single Bag-of-Words (BOW) Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision: 0.99\n",
      "Train Recall: 1.00\n",
      "Train F1-Score: 1.00\n",
      "Validation Precision: 0.85\n",
      "Validation Recall: 0.79\n",
      "Validation F1-Score: 0.82\n"
     ]
    }
   ],
   "source": [
    "#y=1 for clickbait and y=0 for non-clickbait\n",
    "\n",
    "# Create a pipeline with CountVectorizer and MultinomialNB\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training set\n",
    "pipeline.fit(trainData, trainLabel)\n",
    "\n",
    "# Make predictions on the training and validation sets\n",
    "predTrainLabels = pipeline.predict(trainData)\n",
    "predValLabels = pipeline.predict(valData)\n",
    "\n",
    "# Compute precision, recall, and F1-score on the training set\n",
    "trainPrecision = precision_score(trainLabel, predTrainLabels, average='binary')\n",
    "trainRecall = recall_score(trainLabel, predTrainLabels, average='binary')\n",
    "trainF1Score = f1_score(trainLabel, predTrainLabels, average='binary')\n",
    "\n",
    "# Compute precision, recall, and F1-score on the validation set\n",
    "valPrecision = precision_score(valLabel, predValLabels, average='binary')\n",
    "valRecall = recall_score(valLabel, predValLabels, average='binary')\n",
    "valF1Score = f1_score(valLabel, predValLabels, average='binary')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Train Precision: {trainPrecision:.2f}\")\n",
    "print(f\"Train Recall: {trainRecall:.2f}\")\n",
    "print(f\"Train F1-Score: {trainF1Score:.2f}\")\n",
    "\n",
    "print(f\"Validation Precision: {valPrecision:.2f}\")\n",
    "print(f\"Validation Recall: {valRecall:.2f}\")\n",
    "print(f\"Validation F1-Score: {valF1Score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 4 – Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Results:\n",
      "Parameters: {'classifier__alpha': 0.1, 'vectorizer__max_df': 0.5, 'vectorizer__ngram_range': (1, 1)}\n",
      "Precision: 0.84\n",
      "Recall: 0.87\n",
      "F1-Score: 0.85\n",
      "\n",
      "Parameters: {'classifier__alpha': 0.1, 'vectorizer__max_df': 0.5, 'vectorizer__ngram_range': (1, 2)}\n",
      "Precision: 0.84\n",
      "Recall: 0.87\n",
      "F1-Score: 0.85\n",
      "\n",
      "Parameters: {'classifier__alpha': 0.1, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1)}\n",
      "Precision: 0.84\n",
      "Recall: 0.87\n",
      "F1-Score: 0.85\n",
      "\n",
      "Parameters: {'classifier__alpha': 0.1, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 2)}\n",
      "Precision: 0.84\n",
      "Recall: 0.87\n",
      "F1-Score: 0.85\n",
      "\n",
      "Parameters: {'classifier__alpha': 0.1, 'vectorizer__max_df': 1.0, 'vectorizer__ngram_range': (1, 1)}\n",
      "Precision: 0.84\n",
      "Recall: 0.87\n",
      "F1-Score: 0.85\n",
      "\n",
      "Bottom Results:\n",
      "Parameters: {'classifier__alpha': 10.0, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 1)}\n",
      "Precision: 0.92\n",
      "Recall: 0.72\n",
      "F1-Score: 0.81\n",
      "\n",
      "Parameters: {'classifier__alpha': 10.0, 'vectorizer__max_df': 1.0, 'vectorizer__ngram_range': (1, 1)}\n",
      "Precision: 0.92\n",
      "Recall: 0.72\n",
      "F1-Score: 0.81\n",
      "\n",
      "Parameters: {'classifier__alpha': 10.0, 'vectorizer__max_df': 0.5, 'vectorizer__ngram_range': (1, 2)}\n",
      "Precision: 0.91\n",
      "Recall: 0.64\n",
      "F1-Score: 0.75\n",
      "\n",
      "Parameters: {'classifier__alpha': 10.0, 'vectorizer__max_df': 0.75, 'vectorizer__ngram_range': (1, 2)}\n",
      "Precision: 0.91\n",
      "Recall: 0.64\n",
      "F1-Score: 0.75\n",
      "\n",
      "Parameters: {'classifier__alpha': 10.0, 'vectorizer__max_df': 1.0, 'vectorizer__ngram_range': (1, 2)}\n",
      "Precision: 0.91\n",
      "Recall: 0.64\n",
      "F1-Score: 0.75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for grid search\n",
    "parameterGrid = {\n",
    "    'vectorizer__max_df': [0.5, 0.75, 1.0],\n",
    "    'classifier__alpha': [0.1, 1.0, 10.0],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)]\n",
    "}\n",
    "results = []\n",
    "\n",
    "# Iterate over parameter combinations\n",
    "for parameters in ParameterGrid(parameterGrid):\n",
    "    # Create a pipeline with CountVectorizer and MultinomialNB\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(max_df = parameters['vectorizer__max_df'], ngram_range = parameters['vectorizer__ngram_range'])),\n",
    "        ('classifier', MultinomialNB(alpha = parameters['classifier__alpha']))\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline on the training set\n",
    "    pipeline.fit(trainData, trainLabel)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    predValLabels = pipeline.predict(valData)\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(valLabel, predValLabels)\n",
    "    recall = recall_score(valLabel, predValLabels)\n",
    "    f1 = f1_score(valLabel, predValLabels)\n",
    "\n",
    "    results.append({\n",
    "        'params': parameters,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "\n",
    "# Sort the results by F1-score in descending order\n",
    "results.sort(key=lambda x: x['f1_score'], reverse=True)\n",
    "\n",
    "# Print the top and bottom results\n",
    "num_results_to_show = min(5, len(results))\n",
    "print(f\"Top {num_results_to_show} Results:\")\n",
    "for i in range(num_results_to_show):\n",
    "    print(f\"Parameters: {results[i]['params']}\")\n",
    "    print(f\"Precision: {results[i]['precision']:.2f}\")\n",
    "    print(f\"Recall: {results[i]['recall']:.2f}\")\n",
    "    print(f\"F1-Score: {results[i]['f1_score']:.2f}\")\n",
    "    print()\n",
    "\n",
    "if len(results) > 5:\n",
    "    print(\"Bottom Results:\")\n",
    "    for i in range(-5, 0):\n",
    "        print(f\"Parameters: {results[i]['params']}\")\n",
    "        print(f\"Precision: {results[i]['precision']:.2f}\")\n",
    "        print(f\"Recall: {results[i]['recall']:.2f}\")\n",
    "        print(f\"F1-Score: {results[i]['f1_score']:.2f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 5 – Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.80\n",
      "Test Recall: 0.83\n",
      "Test F1-Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "temp1 = results[0]['params']['vectorizer__max_df']\n",
    "temp2 = results[0]['params']['vectorizer__ngram_range']\n",
    "temp3 = results[0]['params']['classifier__alpha']\n",
    "#print(temp1, temp2, temp3)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(max_df = results[0]['params']['vectorizer__max_df'], ngram_range = results[0]['params']['vectorizer__ngram_range'])),\n",
    "        ('classifier', MultinomialNB(alpha = results[0]['params']['classifier__alpha']))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline on the training set\n",
    "pipeline.fit(trainData, trainLabel)\n",
    "\n",
    "# Apply the selected model to the test set\n",
    "predTestLabels = pipeline.predict(testData)\n",
    "\n",
    "# Compute precision, recall, and F1-score on the test set\n",
    "testPrecision = precision_score(testLabel, predTestLabels)\n",
    "testRecall = recall_score(testLabel, predTestLabels)\n",
    "testF1Score = f1_score(testLabel, predTestLabels)\n",
    "\n",
    "# Print the results on the test set\n",
    "print(f\"Test Precision: {testPrecision:.2f}\")\n",
    "print(f\"Test Recall: {testRecall:.2f}\")\n",
    "print(f\"Test F1-Score: {testF1Score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 6 – Key Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Clickbait Indicators:\n",
      "this\n",
      "believe\n",
      "reason\n",
      "know\n",
      "photo\n"
     ]
    }
   ],
   "source": [
    "# Get the MultinomialNB classifier from the selected model\n",
    "classifier = pipeline.named_steps['classifier']\n",
    "\n",
    "# Get the feature log probabilities (log likelihoods)\n",
    "featureLogProbabilities = classifier.feature_log_prob_\n",
    "\n",
    "# Get the vocabulary from the CountVectorizer\n",
    "vectorizer = pipeline.named_steps['vectorizer']\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate the log odds for each feature (word)\n",
    "logOdds = featureLogProbabilities[1] - featureLogProbabilities[0]\n",
    "\n",
    "# Create a dictionary to store words and their log odds\n",
    "wordLogOdds = {word: logOdd for word, logOdd in zip(vocabulary, logOdds)}\n",
    "\n",
    "# Sort words by log odds in descending order\n",
    "sortedWords = sorted(wordLogOdds.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select the top 5 words as strong clickbait indicators\n",
    "clickbaitIndicators = [word for word, _ in sortedWords[:5]]\n",
    "\n",
    "# Print the top 5 clickbait indicators\n",
    "print(\"Top 5 Clickbait Indicators:\")\n",
    "for word in clickbaitIndicators:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 7 – Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8518518518518519\n",
      "Recall: 0.3026315789473684\n"
     ]
    }
   ],
   "source": [
    "top_keywords = clickbaitIndicators\n",
    "\n",
    "# Create a regular expression pattern to match any of the keywords with word boundaries\n",
    "pattern = r'\\b(?:' + '|'.join(re.escape(keyword) for keyword in top_keywords) + r')\\b'\n",
    "\n",
    "# Function to check if the text contains any of the top keywords\n",
    "def checkForClickbait(text):\n",
    "    return re.search(pattern, text, flags=re.IGNORECASE) is not None\n",
    "\n",
    "# Apply the function to your test set and store the results\n",
    "testResults = [checkForClickbait(text) for text in testData]\n",
    "\n",
    "# Calculate the number of true positives, false positives, true negatives, and false negatives\n",
    "tp = sum(1 for pred, actual in zip(testResults, testLabel) if pred and actual == 1)\n",
    "fp = sum(1 for pred, actual in zip(testResults, testLabel) if pred and actual == 0)\n",
    "tn = sum(1 for pred, actual in zip(testResults, testLabel) if not pred and actual == 0)\n",
    "fn = sum(1 for pred, actual in zip(testResults, testLabel) if not pred and actual == 1)\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "# Print the precision and recall\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 8 – Comparing results (15pts – Answer in Blackboard)\n",
    "Compare your rules-based classifier from the previous problem with your machine-learning solution. Which classifier showed the best model metrics? Why do you think it performed the best? How did both compare to your trivial baseline (Problem 2)? If you had more time to try to improve this clickbait detection solution, what would you explore? (There is no single correct answer to this question; review your results and come up with your own ideas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier that showed the best model metrics was the machine-learning solution. I think it performed the best because looking at the metric results for the rules-based classifier, the classifier barely predicted anything which resulted in it correctly predicting the Positive it did predict on but because it barely predicted, there were a lot of False Negatives. The classifier from the machine-learning solutions did significantly better compared to the baseline. We can see this by the machine-learning solutions' Precision. Recall, and F1-Score being higher than that of the baseline. The rules-based classifier did similar compared to the baseline. We can see this by the rules-based having a higher Precision but a lower Recall. urrently, the clickbait detection solution barely predicts the phrase to be clickbait. This can be shown by the Precision being 1 or near 1 and then Recall being near 0. The current method predicts the words that have the highest probability to be clickbait and lowest probability to be non clickbait. Because of this, the words being selected are very unique words that don't appear that often resulting in the testing phrases to rarely containing them If I had more time to try to improve this clickbait detection solution, I would explore try to determine a method that not only finds the words with the highest probability to be clickbait and lowest probability to be non clickbait, but also appear a certain amount of times. Currently if a word appears once in the clickbait and 0 times in non clickbait, it would have a 100% probability of being clickbait however this is not a good identifier. If I could determine a minimum number of times a word appears to be considered an indicator, then the words chosen as indicators would not only have good probabilities of being clickbait and not non clickbiat, but they would also appear more frequently than the current method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
