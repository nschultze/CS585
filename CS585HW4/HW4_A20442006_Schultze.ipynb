{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ncbi-disease' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/spyysalo/ncbi-disease.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pycrfsuite\n",
    "from sklearn.metrics import classification_report\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1 – Reading the data in CoNLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readConllFile(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        currentTokens = []\n",
    "        currentTags = []\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                tokens.append(currentTokens)\n",
    "                tags.append(currentTags)\n",
    "                currentTokens = []\n",
    "                currentTags = []\n",
    "            else:\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    token, tag = parts\n",
    "                    currentTokens.append(token)\n",
    "                    currentTags.append(tag)\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences in train: 5432\n",
      "Number of sequences in test: 940\n",
      "Train Tokens: ['Identification', 'of', 'APC2', ',', 'a', 'homologue', 'of', 'the', 'adenomatous', 'polyposis', 'coli', 'tumour', 'suppressor', '.']\n",
      "Train Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'I-Disease', 'O', 'O']\n",
      "Test Tokens: ['Clustering', 'of', 'missense', 'mutations', 'in', 'the', 'ataxia', '-', 'telangiectasia', 'gene', 'in', 'a', 'sporadic', 'T', '-', 'cell', 'leukaemia', '.']\n",
      "Test Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'I-Disease', 'I-Disease', 'O']\n"
     ]
    }
   ],
   "source": [
    "trainFilePath = \"ncbi-disease/conll/train.tsv\"\n",
    "testFilePath = \"ncbi-disease/conll/test.tsv\"\n",
    "\n",
    "trainTokens, trainTags = readConllFile(trainFilePath)\n",
    "testTokens, testTags = readConllFile(testFilePath)\n",
    "\n",
    "numberTrainSequences = len(trainTokens)\n",
    "numberTestSequences = len(testTokens)\n",
    "\n",
    "print(\"Number of sequences in train:\", numberTrainSequences)\n",
    "print(\"Number of sequences in test:\", numberTestSequences)\n",
    "\n",
    "print(\"Train Tokens:\", trainTokens[0])\n",
    "print(\"Train Tags:\", trainTags[0])\n",
    "print(\"Test Tokens:\", testTokens[0])\n",
    "print(\"Test Tags:\", testTags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2 - Data Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Counts in Training Data:\n",
      "O: 124819\n",
      "B-Disease: 5145\n",
      "I-Disease: 6122\n",
      "\n",
      "Top 20 Words Associated with Disease Tags:\n",
      "-: 636\n",
      "deficiency: 322\n",
      "syndrome: 281\n",
      "cancer: 269\n",
      "disease: 256\n",
      "of: 178\n",
      "dystrophy: 176\n",
      "breast: 151\n",
      "ovarian: 132\n",
      "X: 122\n",
      "and: 120\n",
      "DM: 120\n",
      "ALD: 114\n",
      "DMD: 110\n",
      "APC: 100\n",
      "disorder: 94\n",
      "muscular: 94\n",
      "G6PD: 92\n",
      "linked: 81\n",
      "the: 78\n"
     ]
    }
   ],
   "source": [
    "tagCount = Counter(tag for tagSequence in trainTags for tag in tagSequence)\n",
    "\n",
    "print(\"Tag Counts in Training Data:\")\n",
    "for tag, count in tagCount.items():\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "wordCount = Counter()\n",
    "for i in range(len(trainTokens)):\n",
    "    tokenSequence = trainTokens[i]\n",
    "    tagSequence = trainTags[i]\n",
    "    for j in range(len(tokenSequence)):\n",
    "        if tagSequence[j] in [\"B-Disease\", \"I-Disease\"]:\n",
    "            wordCount[tokenSequence[j]] += 1\n",
    "\n",
    "top20Words = wordCount.most_common(20)\n",
    "\n",
    "print(\"\\nTop 20 Words Associated with Disease Tags:\")\n",
    "for word, count in top20Words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3 - Building features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w0.lower=identification', 'w0.suffix3=ion', 'w-1.word=BOS', 'w+1.word=of', 'wordPrefix=ide', 'wordShape=lowercase', 'hasHyphen=False', 'char2gram=id', 'char2gram=de', 'char2gram=en', 'char2gram=nt', 'char2gram=ti', 'char2gram=if', 'char2gram=fi', 'char2gram=ic', 'char2gram=ca', 'char2gram=at', 'char2gram=ti', 'char2gram=io', 'char2gram=on', 'char3gram=ide', 'char3gram=den', 'char3gram=ent', 'char3gram=nti', 'char3gram=tif', 'char3gram=ifi', 'char3gram=fic', 'char3gram=ica', 'char3gram=cat', 'char3gram=ati', 'char3gram=tio', 'char3gram=ion']\n",
      "['w0.lower=of', 'w0.suffix3=of', 'w-1.word=Identification', 'w+1.word=APC2', 'wordPrefix=of', 'wordShape=lowercase', 'hasHyphen=False', 'char2gram=of']\n",
      "['w0.lower=apc2', 'w0.suffix3=pc2', 'w-1.word=of', 'w+1.word=,', 'wordPrefix=apc', 'wordShape=lowercase', 'hasHyphen=False', 'char2gram=ap', 'char2gram=pc', 'char2gram=c2', 'char3gram=apc', 'char3gram=pc2']\n"
     ]
    }
   ],
   "source": [
    "def wordToFeatures(tokens, position):\n",
    "    features = []\n",
    "    if position >= len(tokens):\n",
    "        return features\n",
    "\n",
    "    currentWord = tokens[position].lower()\n",
    "    \n",
    "    suffix = currentWord[-3:]\n",
    "\n",
    "    previousWord = tokens[position - 1] if position > 0 else \"BOS\"\n",
    "    nextWord = tokens[position + 1] if position < len(tokens) - 1 else \"EOS\"\n",
    "\n",
    "    wordPrefix = currentWord[:3]\n",
    "\n",
    "    if currentWord.islower():\n",
    "        wordShape = \"lowercase\"\n",
    "    elif currentWord.isupper():\n",
    "        wordShape = \"uppercase\"\n",
    "    else:\n",
    "        wordShape = \"mixedcase\"\n",
    "\n",
    "    hasHyphen = '-' in currentWord\n",
    "\n",
    "    char2grams = [currentWord[i:i+2] for i in range(len(currentWord) - 1)]\n",
    "    char3grams = [currentWord[i:i+3] for i in range(len(currentWord) - 2)]\n",
    "\n",
    "    features.extend([\n",
    "        'w0.lower=' + currentWord,\n",
    "        'w0.suffix3=' + suffix,\n",
    "        'w-1.word=' + previousWord,\n",
    "        'w+1.word=' + nextWord,\n",
    "        'wordPrefix=' + wordPrefix,\n",
    "        'wordShape=' + wordShape,\n",
    "        'hasHyphen=' + str(hasHyphen),\n",
    "    ])\n",
    "\n",
    "    features.extend(['char2gram=' + gram for gram in char2grams])\n",
    "    features.extend(['char3gram=' + gram for gram in char3grams])\n",
    "\n",
    "    return features\n",
    "\n",
    "for i in range(3):\n",
    "    features = wordToFeatures(trainTokens[0], i)\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4 - Training a CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-Disease       0.86      0.72      0.79       960\n",
      "   I-Disease       0.84      0.76      0.80      1087\n",
      "           O       0.98      0.99      0.99     22450\n",
      "\n",
      "    accuracy                           0.97     24497\n",
      "   macro avg       0.89      0.83      0.86     24497\n",
      "weighted avg       0.97      0.97      0.97     24497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dataToFeatures(tokens, feature_function):\n",
    "    return [feature_function(tokens, i) for i in range(len(tokens))]\n",
    "\n",
    "trainFeatures = [dataToFeatures(tokens, wordToFeatures) for tokens in trainTokens]\n",
    "testFeatures = [dataToFeatures(tokens, wordToFeatures) for tokens in testTokens]\n",
    "\n",
    "flatTrainTags = list(chain.from_iterable(trainTags))\n",
    "flatTestTags = list(chain.from_iterable(testTags))\n",
    "\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(trainFeatures, trainTags):\n",
    "    trainer.append(xseq, yseq)\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # L1 penalty\n",
    "    'c2': 1e-3,  # L2 penalty\n",
    "    'max_iterations': 50,\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "trainer.train('conll2002-esp.crfsuite')\n",
    "\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('conll2002-esp.crfsuite')\n",
    "\n",
    "predictedTags = [tagger.tag(xseq) for xseq in testFeatures]\n",
    "flattenPredictedTags = list(chain.from_iterable(predictedTags))  # Flatten the list of lists\n",
    "\n",
    "targetNames = [\"B-Disease\", \"I-Disease\", \"O\"]\n",
    "report = classification_report(flatTestTags, flattenPredictedTags, target_names=targetNames)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5 - Inspecting the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayFeatureWeights(model_path):\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open(model_path)\n",
    "\n",
    "    featureWeights = tagger.info().state_features\n",
    "    for feature, weight in featureWeights.items():\n",
    "        print(f\"Feature: {feature} | Weight: {weight}\")\n",
    "\n",
    "model_path = 'conll2002-esp.crfsuite'  # Replace with the path to your CRF model\n",
    "#displayFeatureWeights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Weights for 'hasHyphen' in the model:\n",
      "Feature: ('hasHyphen=False', 'O') | Weight: 0.189586\n",
      "Feature: ('hasHyphen=False', 'B-Disease') | Weight: -0.267843\n",
      "Feature: ('hasHyphen=False', 'I-Disease') | Weight: -0.15311\n",
      "Feature: ('hasHyphen=True', 'O') | Weight: 0.18216\n",
      "Feature: ('hasHyphen=True', 'I-Disease') | Weight: 0.526131\n"
     ]
    }
   ],
   "source": [
    "feature_names = [\"hasHyphen\"]\n",
    "state_features = tagger.info().state_features\n",
    "print(f\"Feature Weights for '{feature_names[0]}' in the model:\")\n",
    "for feature_name in feature_names:\n",
    "    counter = 0\n",
    "    for feature, weight in state_features.items():\n",
    "        if feature_name in feature[0].split(\"=\"):\n",
    "            print(f\"Feature: {feature} | Weight: {weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 6 - Document level performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive:  476\n",
      "False Positive:  16\n",
      "False Negative:  63\n",
      "Document-Level Precision: 0.967479674796748\n",
      "Document-Level Recall: 0.8831168831168831\n"
     ]
    }
   ],
   "source": [
    "def tokens_to_document_labels(token_tags):\n",
    "    return 1 if \"B-Disease\" in token_tags else 0\n",
    "\n",
    "#print(\"Predicted Tags:\", predictedTags[:5])\n",
    "#print(\"Test Tags:\", testTags[:5])\n",
    "\n",
    "trueDocumentLabels = [tokens_to_document_labels(tags) for tags in testTags]\n",
    "#print(\"True: \", trueDocumentLabels)\n",
    "\n",
    "predictedDocumentLabels = [tokens_to_document_labels(tags) for tags in predictedTags]\n",
    "#print(\"Predictions: \", predictedDocumentLabels)\n",
    "\n",
    "truePositive = sum(1 for true_label, predicted_label in zip(trueDocumentLabels, predictedDocumentLabels) if true_label == 1 and predicted_label == 1)\n",
    "falsePositive = sum(1 for true_label, predicted_label in zip(trueDocumentLabels, predictedDocumentLabels) if true_label == 0 and predicted_label == 1)\n",
    "falseNegative = sum(1 for true_label, predicted_label in zip(trueDocumentLabels, predictedDocumentLabels) if true_label == 1 and predicted_label == 0)\n",
    "\n",
    "precision = truePositive / (truePositive + falsePositive) if truePositive + falsePositive > 0 else 0\n",
    "recall = truePositive / (truePositive + falseNegative) if truePositive + falseNegative > 0 else 0\n",
    "\n",
    "print(\"True Positive: \", truePositive)\n",
    "print(\"False Positive: \", falsePositive)\n",
    "print(\"False Negative: \", falseNegative)\n",
    "print(\"Document-Level Precision:\", precision)\n",
    "print(\"Document-Level Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 7 - State Transitions (10 pts – Answer in Blackboard)\n",
    "The python-crfsuite library allows you to set a Boolean hyper-parameter called “feature.possible_transitions”. If this parameter is True, then the model may output tag-to-tag transitions that were never seen in training data. [You do not need to apply this parameter in your code to answer this question]\n",
    "• What is an example of one tag-to-tag transition that never occurred in the training data?\n",
    "• For this particular experiment, do you think it makes sense to set this parameter to True or\n",
    "False? That is, should you allow transitions that never occurred in the training data? Explain your answer briefly.\n",
    "\n",
    "Answer: An example of a tag-to-tag transition that is absent in the training data is the transition from \"O\" to \"I-Disease.\" In the tagging scheme for the data, \"B\" denotes the first word/token in a disease mention, \"I\" indicates subsequent words/tokens within the same mention, and \"O\" signifies that a token is not part of a disease mention. This means that \"I-Disease\" can only occur  after \"B-Disease,\"  and shows that the transition from \"O\" to \"I-Disease\" is never observed. Setting the parameter to True or False depends on the characteristics of your training data. If the training data is comprehensive and covers most tag transitions, it would be best to set the parameter to False. But, if there is a limited amount of training data or unseen transitions, it would be best to set the parameter to True. Because the data is from NBCI Disease Corpus, we can assume that the data is in fact mostly correct and we will not run into a lot of unseen transitions, because of this and because our dataset is comprehensive and cover most tag transitions, I believe it would be best to set the parameter to false."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
