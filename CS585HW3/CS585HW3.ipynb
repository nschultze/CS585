{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1- Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0       hide new secretions from the parental units       0\n",
       "1               contains no wit , only labored gags       0\n",
       "2  that loves its characters and communicates som...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"SST-2\"\n",
    "file_name = \"train.tsv\"\n",
    "\n",
    "file_path = os.path.join(data_dir, file_name)\n",
    "\n",
    "df_sst = pd.read_csv(file_path, delimiter=\"\\t\")\n",
    "df_sst.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: 100\n",
      "Test dataset size: 100\n",
      "Training dataset size: 67149\n"
     ]
    }
   ],
   "source": [
    "validationSize = 100\n",
    "testSize = 100\n",
    "\n",
    "df_sst = df_sst.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "val = df_sst[:validationSize]\n",
    "test = df_sst[validationSize:validationSize + testSize]\n",
    "train = df_sst[validationSize + testSize:]\n",
    "\n",
    "print(\"Validation dataset size:\", len(val))\n",
    "print(\"Test dataset size:\", len(test))\n",
    "print(\"Training dataset size:\", len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Probability: 55.77%\n",
      "Negative Probability: 44.23%\n"
     ]
    }
   ],
   "source": [
    "positiveClassCount = (train['label'] == 1).sum()\n",
    "negativeClassCount = (train['label'] == 0).sum()\n",
    "\n",
    "totalLength = len(train)\n",
    "\n",
    "priorPositiveProbability = (positiveClassCount / totalLength) * 100\n",
    "priorNegativeProbability = (negativeClassCount / totalLength) * 100\n",
    "\n",
    "print(\"Positive Probability: {:.2f}%\".format(priorPositiveProbability))\n",
    "print(\"Negative Probability: {:.2f}%\".format(priorNegativeProbability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2- Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200    [<s>, more, measured, or, polished, production...\n",
      "201    [<s>, a, child, 's, interest, and, an, adult, ...\n",
      "202    [<s>, delivered, dialogue, and, a, heroine, wh...\n",
      "203    [<s>, as, inept, as, big-screen, remakes, of, ...\n",
      "204    [<s>, ,, crocodile, hunter, has, the, hurried,...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def tokenize(df, column):\n",
    "    tokenizedSequences = df[column].apply(lambda sentence: ['<s>'] + sentence.split() + ['</s>'])\n",
    "    return tokenizedSequences\n",
    "\n",
    "tokenizedSequences = tokenize(train, 'sentence')\n",
    "print(tokenizedSequences.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size : 14817\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "for tokenizedSequence in train['sentence']:\n",
    "    tokens = tokenizedSequence.split()\n",
    "    vocabulary.update(tokens)\n",
    "\n",
    "vocabulary.add('<s>')\n",
    "vocabulary.add('</s>')\n",
    "\n",
    "vocabularySize = len(vocabulary)\n",
    "\n",
    "#print(vocabulary)\n",
    "print(\"Vocabulary size :\", vocabularySize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3- Bigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of the bigram ('<s>', 'the') in the training set: 4451\n"
     ]
    }
   ],
   "source": [
    "def countBigramFrequencies(tokenizedSequences):\n",
    "    bigramCounts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for sequence in tokenizedSequences:\n",
    "        for i in range(len(sequence) - 1):\n",
    "            wi = sequence[i]\n",
    "            wj = sequence[i + 1]\n",
    "            bigramCounts[wi][wj] += 1\n",
    "    return bigramCounts\n",
    "\n",
    "bigramCounts = countBigramFrequencies(tokenizedSequences)\n",
    "frequency = bigramCounts['<s>']['the']\n",
    "\n",
    "print(\"Frequency of the bigram ('<s>', 'the') in the training set:\", frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4- Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probability with alpha=0.001: -1.0251860898691059\n",
      "Log Probability with alpha=0.5: -6.173181082203538\n"
     ]
    }
   ],
   "source": [
    "def smoothingLogProbability(wm, wm1, bigramCounts, alpha, vocabularySize):\n",
    "    countWmWm1 = bigramCounts.get(wm1, {}).get(wm, 0) + alpha\n",
    "    countWm1 = sum(bigramCounts.get(wm1, {}).values()) + (alpha * vocabularySize)\n",
    "    logProb = math.log(countWmWm1 / countWm1)\n",
    "    return logProb\n",
    "\n",
    "wordWm1 = \"academy\"\n",
    "wordWm = \"award\"\n",
    "\n",
    "alpha1 = 0.001\n",
    "alpha2 = 0.5\n",
    "\n",
    "logProb1 = smoothingLogProbability(wordWm, wordWm1, bigramCounts, alpha1, vocabularySize)\n",
    "logProb2 = smoothingLogProbability(wordWm, wordWm1, bigramCounts, alpha2, vocabularySize)\n",
    "\n",
    "print(f\"Log Probability with alpha={alpha1}: {logProb1}\")\n",
    "print(f\"Log Probability with alpha={alpha2}: {logProb2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5- Sentence Log-Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probability of Sentence 1: -71.27052642123148\n",
      "Log Probability of Sentence 2: -145.60202109372278\n"
     ]
    }
   ],
   "source": [
    "def logProbability(sentence, bigramCounts, alpha, vocabularySize):\n",
    "    sentence_tokens = sentence.split()\n",
    "    logProb = 0.0\n",
    "\n",
    "    for i in range(1, len(sentence_tokens)):\n",
    "        wm1 = sentence_tokens[i - 1]\n",
    "        wm = sentence_tokens[i]\n",
    "        \n",
    "        logProb += math.log(\n",
    "            (bigramCounts.get(wm1, {}).get(wm, 0) + alpha) /\n",
    "            (sum(bigramCounts.get(wm1, {}).values()) + (alpha * vocabularySize))\n",
    "        )\n",
    "\n",
    "    return logProb\n",
    "\n",
    "alpha = 0.001\n",
    "vocabularySize = len(vocabulary)\n",
    "\n",
    "sentence1 = \"this was a really great movie but it was a little too long.\"\n",
    "sentence2 = \"long too little a was it but movie great really a was this.\"\n",
    "\n",
    "logProb1 = logProbability(sentence1, bigramCounts, alpha, vocabularySize)\n",
    "logProb2 = logProbability(sentence2, bigramCounts, alpha, vocabularySize)\n",
    "\n",
    "print(f\"Log Probability of Sentence 1: {logProb1}\")\n",
    "print(f\"Log Probability of Sentence 2: {logProb2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 6- Tuning Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.001: Log-Likelihood = -4060.954140773797\n",
      "Alpha = 0.01: Log-Likelihood = -4564.418451383303\n",
      "Alpha = 0.1: Log-Likelihood = -5571.937644465253\n",
      "Selected Alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "alphaValues = [0.001, 0.01, 0.1]\n",
    "\n",
    "logLikelihoods = {}\n",
    "\n",
    "for alpha in alphaValues:\n",
    "    logLikelihood = 0.0\n",
    "    for sentence in val['sentence']:\n",
    "        logLikelihood += logProbability(sentence, bigramCounts, alpha, vocabularySize)\n",
    "    logLikelihoods[alpha] = logLikelihood\n",
    "\n",
    "bestAlpha = max(logLikelihoods, key = logLikelihoods.get)\n",
    "\n",
    "for alpha, logLikelihood in logLikelihoods.items():\n",
    "    print(f\"Alpha = {alpha}: Log-Likelihood = {logLikelihood}\")\n",
    "\n",
    "print(f\"Selected Alpha: {bestAlpha}\")\n",
    "\n",
    "selectedAlpha = bestAlpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 7- Applying Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution of Predicted Labels:\n",
      "1    60\n",
      "0    40\n",
      "Name: predicted_label, dtype: int64\n",
      "\n",
      "Accuracy of the Experiment: 88.00%\n",
      "Accuracy of Positive Predictions: 86.36%\n",
      "Accuracy of Negative Predictions: 91.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qz/8_cb5kss6j909hn7j1mwr93c0000gn/T/ipykernel_26106/80856143.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['predicted_label'] = test['sentence'].apply(classify_sentiment)\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_pad_sentence(sentence):\n",
    "    tokens = ['<s>'] + sentence.split() + ['</s>']\n",
    "    return tokens\n",
    "\n",
    "priorPositiveProbability = (train['label'] == 1).mean()\n",
    "priorNegativeProbability = (train['label'] == 0).mean()\n",
    "\n",
    "positive_tokenized = train[train['label'] == 1]['sentence'].apply(tokenize_and_pad_sentence)\n",
    "negative_tokenized = train[train['label'] == 0]['sentence'].apply(tokenize_and_pad_sentence)\n",
    "\n",
    "positive_bigramCounts = countBigramFrequencies(positive_tokenized)\n",
    "negative_bigramCounts = countBigramFrequencies(negative_tokenized)\n",
    "\n",
    "selectedAlpha = bestAlpha\n",
    "\n",
    "def sentiment_score(sentence, bigramCounts, alpha, vocabularySize):\n",
    "    return logProbability(sentence, bigramCounts, alpha, vocabularySize)\n",
    "\n",
    "def classify_sentiment(sentence):\n",
    "    positive_score = sentiment_score(sentence, positive_bigramCounts, selectedAlpha, vocabularySize)\n",
    "    negative_score = sentiment_score(sentence, negative_bigramCounts, selectedAlpha, vocabularySize)\n",
    "    if positive_score > negative_score:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "test['predicted_label'] = test['sentence'].apply(classify_sentiment)\n",
    "predicted_distribution = test['predicted_label'].value_counts()\n",
    "\n",
    "positive_predictions = test[test['label'] == 1]\n",
    "negative_predictions = test[test['label'] == 0]\n",
    "\n",
    "correct_predictions = (test['predicted_label'] == test['label']).sum()\n",
    "positive_correct = (positive_predictions['predicted_label'] == positive_predictions['label']).sum()\n",
    "negative_correct = (negative_predictions['predicted_label'] == negative_predictions['label']).sum()\n",
    "\n",
    "total_sentences = len(test)\n",
    "\n",
    "accuracy = correct_predictions / total_sentences\n",
    "positive_accuracy = positive_correct / len(positive_predictions)\n",
    "negative_accuracy = negative_correct / len(negative_predictions)\n",
    "\n",
    "print(\"Class Distribution of Predicted Labels:\")\n",
    "print(predicted_distribution)\n",
    "print(\"\\nAccuracy of the Experiment: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Accuracy of Positive Predictions: {:.2f}%\".format(positive_accuracy * 100))\n",
    "print(\"Accuracy of Negative Predictions: {:.2f}%\".format(negative_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 8- Markov Assumption\n",
    "\n",
    "Where in this homework did you apply the Markov assumption? Imagine you applied the 2nd-order Markov assumption, using trigrams. Do you think your\n",
    "accuracy results would increase or decrease? Why? Or, if you are not sure, give a benefit or drawback of using trigrams for this experiment. (Note: You do not need to rerun this experiment with trigrams to answer this question.)\n",
    "\n",
    "Answer: I applied the Markov Assumption when calculating the log probability of a sentence using bigrams. Each word's probability is conditioned on the previous word, which is a first-order Markov assumption. If we were to apply a 2nd-order Markov assumption using trigrams, it would mean that each word's probability would be conditioned on the previous two words. This approach would result in a more complex model. The accuracy of the model could potentially increase or decrease. One thing that would determine what happens to the accuracy would be the size of the dataset. If the dataset is small, the model might overfit which would result in bad accuracy, but if the dataset is larger, then the model provide more context and gain more information about the relationships between words which could lead to an increase in accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
